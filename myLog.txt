
Building model...

Computing reachable states... 29 states
Reachable states exploration and model construction done in 0.27 secs.
Sorting reachable states list...

Computing reachable states... 43 states
Reachable states exploration and model construction done in 0.175 secs.
Sorting reachable states list...
Progress:

product construction took 0.304874 s

Time for model construction: 0.777 seconds.
Building reward structure...

Starting expected reachability...
Starting Prob1 (minmax)...
Prob1 (minmax) took 4 iterations and 0.006 seconds.
target=638, inf=0, rest=68
Computing the upper bound where 3.04 is used instead of 0.0
Starting value iteration (maxmin)...
Value iteration (maxmin) took 54 iterations and 0.02 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (maxmin)...
Value iteration (maxmin) took 4 iterations and 0.001 seconds.
Expected reachability took 0.038 seconds.

Value in the initial state: 304.0

Time for model checking: 0.06 seconds.

Result: 304.0 (value in the initial state)
Building reward structure...

Starting expected reachability...
Starting Prob1 (minmax)...
Prob1 (minmax) took 4 iterations and 0.0 seconds.
target=638, inf=0, rest=68
Computing the upper bound where 0.9500000000000001 is used instead of 0.0
Starting value iteration (maxmin)...
Value iteration (maxmin) took 54 iterations and 0.017 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (maxmin)...
Value iteration (maxmin) took 4 iterations and 0.0 seconds.
Expected reachability took 0.02 seconds.

Value in the initial state: 95.0

Time for model checking: 0.03 seconds.

Result: 95.0 (value in the initial state)
Building reward structure...

Starting expected reachability...
Starting Prob1 (minmax)...
Prob1 (minmax) took 4 iterations and 0.001 seconds.
target=680, inf=0, rest=26
Computing the upper bound where 6.04 is used instead of 0.0
Starting value iteration (maxmin)...
Value iteration (maxmin) took 54 iterations and 0.039 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (maxmin)...
Value iteration (maxmin) took 4 iterations and 0.0 seconds.
Expected reachability took 0.042 seconds.

Value in the initial state: 604.0

Time for model checking: 0.047 seconds.

Result: 604.0 (value in the initial state)
Building reward structure...

Starting expected reachability...
Starting Prob1 (minmax)...
Prob1 (minmax) took 4 iterations and 0.001 seconds.
target=680, inf=0, rest=26
Computing the upper bound where 0.9 is used instead of 0.0
Starting value iteration (maxmin)...
Value iteration (maxmin) took 54 iterations and 0.008 seconds.
Computed an over-approximation of the solution (in 0 seconds), this will now be used to get the solution
Starting value iteration (maxmin)...
Value iteration (maxmin) took 4 iterations and 0.0 seconds.
Expected reachability took 0.034 seconds.

Value in the initial state: 90.0

Time for model checking: 0.045 seconds.

Result: 90.0 (value in the initial state)
//////////////////////////////////////////////////////////////////////////////
//                   STARTING COMPOSITIONAL MODEL CHECKING                  //
//////////////////////////////////////////////////////////////////////////////

Building Model ... 

Computing reachable states... 29 states
Reachable states exploration and model construction done in 0.221 secs.
Sorting reachable states list...

Computing reachable states... 43 states
Reachable states exploration and model construction done in 0.201 secs.
Sorting reachable states list...

Model checking : "q1": <<1>> (((R{"c1_ru"}<=MAXRUA [ C ]=>R{"c2_ru"}<=MAXRUB [ C ])))

Reducing multi-objective query to CNF: R{"c1_ru"}>MAXRUA [ C ]|R{"c2_ru"}<=MAXRUB [ C ]
expr: [[R{"c1_ru"}>MAXRUA [ C ], R{"c2_ru"}<=MAXRUB [ C ]]]

Warning: Strict inequalities ignored and turned into nonstrict inequalities:
	R{"c1_ru"}>MAXRUA [ C ]

/////////////////   NEW (DIRECT) MODEL CHECKING TASK     /////////////////////
Property:
	[[R{"c1_ru"}>MAXRUA [ C ], R{"c2_ru"}<=MAXRUB [ C ]]]

initial state: 10
operation: Strategy generation
Strategy construction took 0.146211 s
Synthesis took 0.270417 s
strategy: $SU.strat-v0.1
// Stochastic Memory Update Strategy
start strategy
States:
29
// Initial state
InitState:
10
// initial distribution
Init:
{0=1.0}
// next state function
// note: only P1 states
Next:
// first index: current state
// second index: current corner
0 0 {0=1.0}
1 0 {0=1.0}
2 0 {0=1.0}
// memory update function: player states
MemUpdStates:
// first index: current state
// second index: current corner
// third index: next move
0 0 0 {0=1.0}
1 0 0 {0=1.0}
2 0 0 {0=1.0}
10 0 0 {0=1.0}
11 0 0 {0=1.0}
11 0 1 {0=1.0}
11 0 2 {0=1.0}
11 0 3 {0=1.0}
11 0 4 {0=1.0}
11 0 5 {0=1.0}
11 0 6 {0=1.0}
11 0 7 {0=1.0}
11 0 8 {0=1.0}
19 0 0 {0=1.0}
20 0 0 {0=1.0}
21 0 0 {0=1.0}
// memory update function: moves
MemUpdMoves:
// first index: current state
// second index: current move
// third index: curent corner (at move)
// fourth index: next state
0 0 0 11 {0=1.0}
1 0 0 20 {0=1.0}
2 0 0 21 {0=1.0}
10 0 0 19 {0=1.0}
11 0 0 20 {0=1.0}
11 0 0 21 {0=1.0}
11 1 0 20 {0=1.0}
11 1 0 21 {0=1.0}
11 2 0 20 {0=1.0}
11 2 0 21 {0=1.0}
11 3 0 20 {0=1.0}
11 3 0 21 {0=1.0}
11 4 0 20 {0=1.0}
11 4 0 21 {0=1.0}
11 5 0 20 {0=1.0}
11 5 0 21 {0=1.0}
11 6 0 20 {0=1.0}
11 6 0 21 {0=1.0}
11 7 0 20 {0=1.0}
11 7 0 21 {0=1.0}
11 8 0 20 {0=1.0}
19 0 0 0 {0=1.0}
20 0 0 1 {0=1.0}
21 0 0 2 {0=1.0}
Info:

maximum C-iterations: 500
	relative termination threshold: 0.010000
	bounding box: 

maximum D-iterations: 500
	D-iteration offset: 1
endstrategy


Number of states satisfying "q1": 3

Property satisfied in 1 of 1 initial states.

Time for model checking: 0.36800000000000005 seconds.

Result: true (property satisfied in the initial state)

Model checking : "q2": <<1>> (((R{"c1_rt"}<=MAXRTA [ C ]=>R{"c2_rt"}<=MAXRTB [ C ])))

Reducing multi-objective query to CNF: R{"c1_rt"}>MAXRTA [ C ]|R{"c2_rt"}<=MAXRTB [ C ]
expr: [[R{"c1_rt"}>MAXRTA [ C ], R{"c2_rt"}<=MAXRTB [ C ]]]

Warning: Strict inequalities ignored and turned into nonstrict inequalities:
	R{"c1_rt"}>MAXRTA [ C ]

/////////////////   NEW (DIRECT) MODEL CHECKING TASK     /////////////////////
Property:
	[[R{"c1_rt"}>MAXRTA [ C ], R{"c2_rt"}<=MAXRTB [ C ]]]

initial state: 17
operation: Strategy generation
Strategy construction took 0.015881 s
Synthesis took 0.079952 s
strategy: $SU.strat-v0.1
// Stochastic Memory Update Strategy
start strategy
States:
43
// Initial state
InitState:
17
// initial distribution
Init:
{0=1.0}
// next state function
// note: only P1 states
Next:
// first index: current state
// second index: current corner
0 0 {0=1.0}
1 0 {0=1.0}
2 0 {0=1.0}
// memory update function: player states
MemUpdStates:
// first index: current state
// second index: current corner
// third index: next move
0 0 0 {0=1.0}
1 0 0 {0=1.0}
2 0 0 {0=1.0}
17 0 0 {0=1.0}
18 0 0 {0=1.0}
18 0 1 {0=1.0}
26 0 0 {0=1.0}
27 0 0 {0=1.0}
28 0 0 {0=1.0}
// memory update function: moves
MemUpdMoves:
// first index: current state
// second index: current move
// third index: curent corner (at move)
// fourth index: next state
0 0 0 18 {0=1.0}
1 0 0 27 {0=1.0}
2 0 0 28 {0=1.0}
17 0 0 26 {0=1.0}
18 0 0 27 {0=1.0}
18 1 0 27 {0=1.0}
18 1 0 28 {0=1.0}
26 0 0 0 {0=1.0}
27 0 0 1 {0=1.0}
28 0 0 2 {0=1.0}
Info:

maximum C-iterations: 500
	relative termination threshold: 0.010000
	bounding box: 

maximum D-iterations: 500
	D-iteration offset: 1
endstrategy


Number of states satisfying "q2": 43 (all in model)

Property satisfied in 1 of 1 initial states.

Time for model checking: 0.09000000000000001 seconds.

Result: true (property satisfied in the initial state)
//////////////////////////////////////////////////////////////////////////////
//                   COMPLETED COMPOSITIONAL MODEL CHECKING                 //
//////////////////////////////////////////////////////////////////////////////


Model checking completed in 0.91 secs.

Result: true

Reducing multi-objective query to CNF: R{"c1_rt"}<=MAXRTA [ C ]&R{"c1_ru"}<=MAXRUA [ C ]
expr: [[R{"c1_rt"}<=MAXRTA [ C ]], [R{"c1_ru"}<=MAXRUA [ C ]]]
/////////////////   NEW (DIRECT) MODEL CHECKING TASK     /////////////////////
Property:
	[[R{"c1_rt"}<=MAXRTA [ C ]], [R{"c1_ru"}<=MAXRUA [ C ]]]

initial state: 0
operation: Pareto set computation
Pareto set computation took 1.113761 s
Resulting Pareto set:
maxcorners=3. state 0:
[r:[0.0000, 1.0000]r:[1.0000, 0.0000][304.0100, 95.0100]]

Pareto set computation result evaluated
Time for model checking: 1.35 seconds.

Result: Pareto Set

Reducing multi-objective query to CNF: R{"c2_rt"}<=MAXRTB [ C ]&R{"c2_ru"}<=MAXRUB [ C ]
expr: [[R{"c2_rt"}<=MAXRTB [ C ]], [R{"c2_ru"}<=MAXRUB [ C ]]]
/////////////////   NEW (DIRECT) MODEL CHECKING TASK     /////////////////////
Property:
	[[R{"c2_rt"}<=MAXRTB [ C ]], [R{"c2_ru"}<=MAXRUB [ C ]]]

initial state: 0
operation: Pareto set computation
Pareto set computation took 1.828112 s
Resulting Pareto set:
maxcorners=3. state 0:
[r:[0.0000, 1.0000]r:[1.0000, 0.0000][601.0100, 55.0100]]

Pareto set computation result evaluated
Time for model checking: 2.0370000000000004 seconds.

Result: Pareto Set
